{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `value_iteration`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Description**\n",
    "\n",
    "The value_iteration python package provides an implementation of the value iteration algorithm. This algorithm is a well-known solution method for finding the optimal policy of a Markov Decision Process (MDP). \n",
    "\n",
    "For a range of problems that can be modelled as MDPs, the value iteration algorithm finds the optimal policy by maximising a value function iteratively. \n",
    "\n",
    "To learn more about MDPs and the value iteration algorithm, the user can refer to Sections [9.5](https://artint.info/2e/html2e/ArtInt2e.Ch9.S5.html) and [9.5.1](https://artint.info/2e/html2e/ArtInt2e.Ch9.S5.SS1.html) of [Artificial Intelligence: Foundations and Computational Agents 2nd edition](https://artint.info/2e/html2e/ArtInt2e.html)\n",
    "\n",
    "**Key Features**\n",
    "- Implements the value iteration function described by the pseudocode in Figure 9.16 within [Section 9.5.2](https://artint.info/2e/html2e/ArtInt2e.Ch9.S5.SS2.html) of [Artificial Intelligence: Foundations and Computational Agents 2nd edition](https://artint.info/2e/html2e/ArtInt2e.html).\n",
    "- Includes an example of a GridWorld problem which is solved using the `value_iteration` function included in this package.\n",
    "- Can be used for any MDP with appropriate definitions of states, actions, transition probabilities, reward functions, discount factors, and termination conditions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Installation**\n",
    "\n",
    "This python package can be installed from github with pip\n",
    "#### installing from github with pip\n",
    "```python -m pip install \"git+https://github.com/turtlesoul25/value_iteration\"```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple 2-state MDP example\n",
    "This example is taken from [Example 9.27](https://artint.info/2e/html2e/ArtInt2e.Ch9.S5.html#Ch9.Thmciexamplered27) from [Artificial Intelligence: Foundations and Computational Agents 2nd edition](https://artint.info/2e/html2e/ArtInt2e.html). \n",
    "\n",
    "Suppose Sam wanted to make an informed decision about whether to party or relax over the weekend. Sam prefers to party, but is worried about getting sick. Such a problem can be modeled as an MDP with two states, _healthy_ and _sick_, and two actions, _relax_ and _party_. Thus, the set of states is \n",
    "$$S = \\{\\textit{healthy}, \\textit{sick} \\}$$\n",
    "\n",
    "The set of actions is \n",
    "$$A = \\{\\textit{relax}, \\textit{party} \\}$$\n",
    "\n",
    "Based on experience, Sam estimates that $P(s' \\vert s, a)$ is given by \n",
    "\n",
    "| S   | A  | Probability of $s' =$ _healthy_   |\n",
    "|----|----|---------------------------------------------|\n",
    "| healthy  | relax   | 0.95  |\n",
    "| healthy  | party   | 0.7  |\n",
    "| sick  | relax   | 0.5  |\n",
    "| sick  | party   | 0.1  |\n",
    "\n",
    "Sam estimates his immediate rewards to be:\n",
    "\n",
    "| S   | A  | Reward   |\n",
    "|----|----|---------------------------------------------|\n",
    "| healthy  | relax   | 7  |\n",
    "| healthy  | party   | 10  |\n",
    "| sick  | relax   | 0  |\n",
    "| sick  | party   | 2  |\n",
    "\n",
    "\n",
    "Thus, Sam always enjoys partying more than relaxing. However, Sam feels much better overall when healthy, and partying results in being sick more than relaxing does.\n",
    "\n",
    "**The problem is to determine what Sam should do each weekend.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "S = {\"sick\", \"healthy\"} # set of states\n",
    "A = {\"relax\", \"party\"}  # set of actions\n",
    "def P(sp, s, a): # function to calculate transition probabilities\n",
    "    if s == \"healthy\":\n",
    "        if a == \"relax\":\n",
    "            if sp == \"healthy\":\n",
    "                return 0.95\n",
    "            else:\n",
    "                return 0.05\n",
    "            \n",
    "        elif a == \"party\":\n",
    "            if sp == \"healthy\":\n",
    "                return 0.7\n",
    "            else:\n",
    "                return 0.3\n",
    "            \n",
    "    elif s == \"sick\":\n",
    "        if a == \"relax\":\n",
    "            return 0.5\n",
    "        \n",
    "        elif a == \"party\":\n",
    "            if sp == \"healthy\":\n",
    "                return 0.1\n",
    "            else:\n",
    "                return 0.9\n",
    "            \n",
    "\n",
    "def R(s, a): # function to calculate rewards for a state, action pair\n",
    "    if s == \"healthy\":\n",
    "        if a == \"relax\":\n",
    "            return 7\n",
    "        elif a == \"party\":\n",
    "            return 10\n",
    "        \n",
    "    elif s == \"sick\":\n",
    "        if a == \"relax\":\n",
    "            return 0\n",
    "        elif a == \"party\":\n",
    "            return 2\n",
    "\n",
    "# Implementing the value iteration algorithm to solve the MDP\n",
    "results = Value_iteration(S, A, P, R, 0.8, 1000)    # implement algorithm\n",
    "optimal_policy = results[\"optimal_policy\"]          # optimal policy\n",
    "value_function = results[\"value_function\"]          # final value function\n",
    "\n",
    "for s in S: # print results\n",
    "    print(f\"If {s}, then Sam should {optimal_policy[s]}.\")\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
